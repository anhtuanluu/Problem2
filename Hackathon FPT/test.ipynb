{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "my_dict = {'positive': 6395, 'neutral': 2690, 'negative': 1794}\n",
    "my_dict2 = {'positive':6395,'neutral':1345,'negative':598}\n",
    "\n",
    "label_1 = {'unrelate':24850, 'relate':8338}\n",
    "fig, ax = plt.subplots(figsize=(17, 7))\n",
    "keys = list(my_dict.keys())\n",
    "# get values in the same order as keys, and parse percentage values\n",
    "vals = [int(my_dict[k]) for k in keys]\n",
    "sns.barplot(x=keys, y=vals )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup,AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from underthesea import word_tokenize\n",
    "import re\n",
    "#Thay biểu cảm bằng word\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "import os\n",
    "import warnings\n",
    "from transformers import BertForSequenceClassification, AdamW,RobertaForSequenceClassification,AutoModelForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(r\"C:\\Users\\Admin\\Desktop\\Code\\DoAn\\VnCoreNLP\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "def tokenizer_underthesea(text):\n",
    "    return word_tokenize(text, format=\"text\")\n",
    "def tokenizer_vncoreNLP(text):\n",
    "    word_segmented_text = annotator.tokenize(text) \n",
    "    result = ''\n",
    "    for sentences in word_segmented_text:\n",
    "        result = result + ' '.join(i for i in sentences)\n",
    "        result = result+' '\n",
    "    return result.strip()\n",
    "# Thay thế tiền bằng 'giá' và hag_tag\n",
    "def replace_money(text):\n",
    "    for i in range(len(text.split(\" \"))):\n",
    "        dict = {}\n",
    "        dict['k'] = str(text.split(\" \")[i]).find('k')\n",
    "        dict['d'] = str(text.split(\" \")[i]).find('d')\n",
    "        dict['đ'] = str(text.split(\" \")[i]).find('đ')\n",
    "        dict['$'] = str(text.split(\" \")[i]).find('$')\n",
    "        \n",
    "        for j in dict:\n",
    "            if dict[j]==-1:\n",
    "                continue\n",
    "            if len(text.split(\" \")[i])>1 and text.split(\" \")[i][dict[j]-1].isdigit()==True:      \n",
    "                text = text.replace(text.split(\" \")[i],'giá tiền')\n",
    "                break\n",
    "    return text\n",
    "\n",
    "def replace_hagtag(text):\n",
    "    for i in range(len(text.split(\" \"))):\n",
    "        if len(text.split(\" \")[i])<2:\n",
    "            continue\n",
    "        if text.split(\" \")[i][0] == '#' and text.split(\" \")[i][1] != '#':\n",
    "            text = text.replace(text.split(\" \")[i],'hagtag')\n",
    "    return text\n",
    "def remove_character(text):\n",
    "    # Xóa tất cả dấu chấm, phẩy, chấm phẩy, chấm thang, ... trong câu\n",
    "    text = text.replace('\"', \" \").replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"”\", \" \").replace('•','') \\\n",
    "        .replace(\"'\", \" \").replace(\"~\",\"\") \\\n",
    "        .replace(\"-\", \" \").replace(\"_\",\" \").replace(\"*\",\" \").replace(\"@\",\" \").replace(\"|\",\" \")\\\n",
    "        .replace(\"%\",\" \").replace(\"^\",\" \").replace(\"&\",\" \").replace(\"(\",\" \").replace(\")\",\" \")\\\n",
    "        .replace(\"<\",\" \").replace(\">\",\" \").replace(\"+\",\" \").replace(\"-\",\" \").replace(\"/\",\" \").replace(\"_\",\" \").replace(\"[\",\" \").replace(\"]\",\" \").replace(\"#\", \" \").replace(\"$\",\"\")\n",
    "    text = text.strip().lower()\n",
    "    return text\n",
    "\n",
    "def replace_emoticon(text):\n",
    "    for i, j in EMOTICONS_EMO.items():\n",
    "        text = text.replace(i, ' ')\n",
    "    return text\n",
    "\n",
    "def replace_emoji(text):\n",
    "    for i, j in UNICODE_EMOJI.items():\n",
    "        text = text.replace(i, ' ')\n",
    "    return text\n",
    "\n",
    "# #Xoa link trong cau\n",
    "# def delete_link(text):\n",
    "#     return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "def count_word(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens = False))\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def replace_all(text):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def remove_space(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "dict_word_match = {\"mik\":\"mình\",\n",
    "                \" k \":\" không \",\n",
    "                \" ko \":\" không \",\n",
    "                \"size\":\"kích cỡ\",\n",
    "                \"nhug\":\"nhưng\",\n",
    "                \"nhg\":\"nhưng\",\n",
    "                \"bik\":\"biết\",\n",
    "                \"hjhj\":\"hihi\",\n",
    "                \"sp\":\"sản phẩm\",\n",
    "                \"k \":\"không \",\n",
    "                \"siu\":\"siêu\",\n",
    "                \"qc\":\"quảng cáo\",\n",
    "                \"mk\":\"mình\",\n",
    "                \" mh \":\" mình \",\n",
    "                \"cx\":\"cũng\",\n",
    "                \"xog\":\"xong\",\n",
    "                'ship': 'vận chuyển',\n",
    "                'shop': 'cửa hàng',\n",
    "                ' m ': ' mình ',\n",
    "                'mik': 'mình',\n",
    "                'khong': 'không',\n",
    "                'khg': 'không',\n",
    "                'tl': 'trả lời',\n",
    "                'rep': 'trả lời',\n",
    "                ' r ': ' rồi ',\n",
    "                'fb': 'facebook',\n",
    "                'face': 'faceook',\n",
    "                'thanks': 'cảm ơn',\n",
    "                'thank': 'cảm ơn',\n",
    "                'tks': 'cảm ơn',\n",
    "                'tk': 'cảm ơn',\n",
    "                'ok': 'tốt',\n",
    "                'oki': 'tốt',\n",
    "                'okie': 'tốt',\n",
    "                'sp': 'sản phẩm',\n",
    "                'dc': 'được',\n",
    "                'vs': 'với',\n",
    "                'đt': 'điện thoại',\n",
    "                'thjk': 'thích',\n",
    "                'thik': 'thích',\n",
    "                'qá': 'quá',\n",
    "                'trể': 'trễ',\n",
    "                'bgjo': 'bao giờ',\n",
    "                ' h ': ' giờ ',\n",
    "                'qa': 'quá',\n",
    "                'dep': 'đẹp',\n",
    "                'xau': 'xấu',\n",
    "                'ib': 'nhắn tin',\n",
    "                'cute': 'dễ thương',\n",
    "                'sz': 'size',\n",
    "                'good': 'tốt',\n",
    "                'god': 'tốt',\n",
    "                'bt': 'bình thường',\n",
    "                'nvay':'như vậy'\n",
    "                            }\n",
    "def replace_all_1(text):\n",
    "    for i, j in dict_word_match.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def remove_long_word(text):\n",
    "    return re.sub(r'([A-Z])\\1+', lambda m: m.group(1).lower(), text, flags=re.IGNORECASE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>giải_trí</th>\n",
       "      <th>lưu_trú</th>\n",
       "      <th>nhà_hàng</th>\n",
       "      <th>ăn_uống</th>\n",
       "      <th>di_chuyển</th>\n",
       "      <th>mua_sắm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>đi quy nhơn cứ lo không có piza ăn nhưng nhờ c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>các món ăn_ở đây không có gì đặc_sắc , có_lẽ n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lần thứ 2 quay lại đây , vì giới_thiệu quán cà...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rất khuyên bạn nên ghé thăm quán bar thể_thao này</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>ngon , giá hợp_lý không thích món gà rán vì ăn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>người lái_xe đến sớm để đợi tôi . tôi yên_tâm ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>phòng đẹp , sạch_sẽ đầy_đủ tiện_nghi , nằm tro...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>nói_chung là rất nhàm_chán cới kiểu làm du_lịc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3688 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  giải_trí  lưu_trú  \\\n",
       "0     bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...         0        0   \n",
       "1     đi quy nhơn cứ lo không có piza ăn nhưng nhờ c...         0        0   \n",
       "2     các món ăn_ở đây không có gì đặc_sắc , có_lẽ n...         0        0   \n",
       "3     lần thứ 2 quay lại đây , vì giới_thiệu quán cà...         0        0   \n",
       "4     rất khuyên bạn nên ghé thăm quán bar thể_thao này         3        0   \n",
       "...                                                 ...       ...      ...   \n",
       "3683  ngon , giá hợp_lý không thích món gà rán vì ăn...         0        0   \n",
       "3684  người lái_xe đến sớm để đợi tôi . tôi yên_tâm ...         5        0   \n",
       "3685  phòng đẹp , sạch_sẽ đầy_đủ tiện_nghi , nằm tro...         0        5   \n",
       "3686  nói_chung là rất nhàm_chán cới kiểu làm du_lịc...         1        0   \n",
       "3687  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...         0        0   \n",
       "\n",
       "      nhà_hàng  ăn_uống  di_chuyển  mua_sắm  \n",
       "0            0        5          0        0  \n",
       "1            0        5          0        0  \n",
       "2            1        1          0        0  \n",
       "3            0        2          0        0  \n",
       "4            0        0          0        0  \n",
       "...        ...      ...        ...      ...  \n",
       "3683         0        5          0        0  \n",
       "3684         0        0          0        0  \n",
       "3685         0        0          0        0  \n",
       "3686         0        0          0        0  \n",
       "3687         0        0          4        0  \n",
       "\n",
       "[3688 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Admin\\Desktop\\Code\\DoAn\\hackathon.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo data Relate và Unrelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_column(name_column):\n",
    "    column = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(0,6):\n",
    "            column.append(data[name_column][i])\n",
    "    return column\n",
    "text1 = []\n",
    "for i in range(len(data)):\n",
    "    aspect = ['giải_trí','lưu_trú','nhà_hàng','ăn_uống','di_chuyển','mua_sắm']\n",
    "    text1.extend(aspect)\n",
    "text = make_column('Text')\n",
    "giai_tri = make_column('giải_trí')\n",
    "luu_tru = make_column('lưu_trú')\n",
    "nha_hang = make_column('nhà_hàng')\n",
    "an_uong = make_column('ăn_uống')\n",
    "di_chuyen = make_column('di_chuyển')\n",
    "mua_sam = make_column('mua_sắm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text1</th>\n",
       "      <th>giải_trí</th>\n",
       "      <th>lưu_trú</th>\n",
       "      <th>nhà_hàng</th>\n",
       "      <th>ăn_uống</th>\n",
       "      <th>di_chuyển</th>\n",
       "      <th>mua_sắm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>giải_trí</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>lưu_trú</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>nhà_hàng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>ăn_uống</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>di_chuyển</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>lưu_trú</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22124</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>nhà_hàng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>ăn_uống</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22126</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>di_chuyển</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>mua_sắm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22128 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text      Text1  giải_trí  \\\n",
       "0      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...   giải_trí         0   \n",
       "1      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...    lưu_trú         0   \n",
       "2      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...   nhà_hàng         0   \n",
       "3      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...    ăn_uống         0   \n",
       "4      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...  di_chuyển         0   \n",
       "...                                                  ...        ...       ...   \n",
       "22123  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    lưu_trú         0   \n",
       "22124  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...   nhà_hàng         0   \n",
       "22125  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    ăn_uống         0   \n",
       "22126  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...  di_chuyển         0   \n",
       "22127  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    mua_sắm         0   \n",
       "\n",
       "       lưu_trú  nhà_hàng  ăn_uống  di_chuyển  mua_sắm  \n",
       "0            0         0        5          0        0  \n",
       "1            0         0        5          0        0  \n",
       "2            0         0        5          0        0  \n",
       "3            0         0        5          0        0  \n",
       "4            0         0        5          0        0  \n",
       "...        ...       ...      ...        ...      ...  \n",
       "22123        0         0        0          4        0  \n",
       "22124        0         0        0          4        0  \n",
       "22125        0         0        0          4        0  \n",
       "22126        0         0        0          4        0  \n",
       "22127        0         0        0          4        0  \n",
       "\n",
       "[22128 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['Text'] = text\n",
    "data['Text1'] = text1\n",
    "data['giải_trí'] = giai_tri\n",
    "data['lưu_trú'] = luu_tru\n",
    "data['nhà_hàng'] = nha_hang\n",
    "data['ăn_uống'] = an_uong\n",
    "data['di_chuyển'] = di_chuyen\n",
    "data['mua_sắm'] = mua_sam\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check liên quan hay ko liên quan\n",
    "label1 = []\n",
    "for i in range(len(data)):\n",
    "    if data[data['Text1'][i]][i] != 0:\n",
    "        label1.append('relate')\n",
    "    else:\n",
    "        label1.append('unrelate')\n",
    "data['Label1'] = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ các câu dài\n",
    "data['Pair'] = data['Text']+\" \"+data['Text1']\n",
    "data['Count'] = data['Pair'].apply(count_word)\n",
    "data = data[(data['Count']<=254)&(data['Count']>0)].reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text1</th>\n",
       "      <th>giải_trí</th>\n",
       "      <th>lưu_trú</th>\n",
       "      <th>nhà_hàng</th>\n",
       "      <th>ăn_uống</th>\n",
       "      <th>di_chuyển</th>\n",
       "      <th>mua_sắm</th>\n",
       "      <th>Label1</th>\n",
       "      <th>Pair</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>giải_trí</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>lưu_trú</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>nhà_hàng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>ăn_uống</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relate</td>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>di_chuyển</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21973</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>lưu_trú</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>nhà_hàng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>ăn_uống</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>di_chuyển</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>relate</td>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>mua_sắm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>unrelate</td>\n",
       "      <td>lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21978 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text      Text1  giải_trí  \\\n",
       "0      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...   giải_trí         0   \n",
       "1      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...    lưu_trú         0   \n",
       "2      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...   nhà_hàng         0   \n",
       "3      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...    ăn_uống         0   \n",
       "4      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...  di_chuyển         0   \n",
       "...                                                  ...        ...       ...   \n",
       "21973  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    lưu_trú         0   \n",
       "21974  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...   nhà_hàng         0   \n",
       "21975  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    ăn_uống         0   \n",
       "21976  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...  di_chuyển         0   \n",
       "21977  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...    mua_sắm         0   \n",
       "\n",
       "       lưu_trú  nhà_hàng  ăn_uống  di_chuyển  mua_sắm    Label1  \\\n",
       "0            0         0        5          0        0  unrelate   \n",
       "1            0         0        5          0        0  unrelate   \n",
       "2            0         0        5          0        0  unrelate   \n",
       "3            0         0        5          0        0    relate   \n",
       "4            0         0        5          0        0  unrelate   \n",
       "...        ...       ...      ...        ...      ...       ...   \n",
       "21973        0         0        0          4        0  unrelate   \n",
       "21974        0         0        0          4        0  unrelate   \n",
       "21975        0         0        0          4        0  unrelate   \n",
       "21976        0         0        0          4        0    relate   \n",
       "21977        0         0        0          4        0  unrelate   \n",
       "\n",
       "                                                    Pair  Count  \n",
       "0      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...     98  \n",
       "1      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...     98  \n",
       "2      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...     98  \n",
       "3      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...     98  \n",
       "4      bánh rất nhiều tôm to , tôm giòn nằm chễm_chệ ...     98  \n",
       "...                                                  ...    ...  \n",
       "21973  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...     18  \n",
       "21974  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...     18  \n",
       "21975  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...     18  \n",
       "21976  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...     18  \n",
       "21977  lần đầu trải_nghiệm sbay phù_cát , các bạn nhâ...     18  \n",
       "\n",
       "[21978 rows x 11 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_full_hackathon.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[:16362]\n",
    "data_val = data[16362:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')  \n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, with_labels=True,tokenizer=tokenizer):\n",
    "\n",
    "        self.data = data  # pandas dataframe\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels \n",
    "        self.dict_label = {'relate':1,'unrelate':0}\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent = str(self.data.loc[index, 'Text'])\n",
    "        sent1 = str(self.data.loc[index, 'Text1'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent, \n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,  # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "        \n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label = self.data.loc[index, 'Label1']\n",
    "            label = self.dict_label[label]\n",
    "            return token_ids, attn_masks, token_type_ids, label  \n",
    "        else:\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    }
   ],
   "source": [
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(data=data_train, maxlen=258, tokenizer=tokenizer)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(data=data_val, maxlen=258, tokenizer=tokenizer)\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=16)\n",
    "val_loader = DataLoader(val_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Model_Detection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Detection, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    def forward(self, pair_token_ids, token_type_ids, attention_mask):\n",
    "        output_1 = self.roberta(input_ids=pair_token_ids,token_type_ids=token_type_ids,attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model_Detection().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 134,999,810 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                      lr=1e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#134,999,810    135,590,402     137,359,874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17553bfbcc440548ac470b27c943e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.3020\n",
      " Accuracy: 0.8817\n",
      " F1 score: 0.7303\n",
      " F1 full:  [0.92964131 0.53105693]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c8faed973341269988510f06270d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2343\n",
      " Accuracy: 0.9197\n",
      " F1 score: 0.8475\n",
      " F1 full:  [0.95127844 0.74381894]\n",
      "00:05:38.88\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0903c4ac4e64453ac9f150db84c35e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.1870\n",
      " Accuracy: 0.9319\n",
      " F1 score: 0.8824\n",
      " F1 full:  [0.95763328 0.80711782]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66ce636e6974d00bce99ff1017c3dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2047\n",
      " Accuracy: 0.9229\n",
      " F1 score: 0.8568\n",
      " F1 full:  [0.95299891 0.76053227]\n",
      "00:05:51.37\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f17f01b654c47058ef7572e3a2a7571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.1564\n",
      " Accuracy: 0.9438\n",
      " F1 score: 0.9028\n",
      " F1 full:  [0.96491804 0.8406554 ]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d034ac8799f4698af55abd64afd1899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2035\n",
      " Accuracy: 0.9272\n",
      " F1 score: 0.8744\n",
      " F1 full:  [0.95482941 0.79395687]\n",
      "00:05:50.81\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c23bda8f6874c0f9bbff5bffa54ef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.1286\n",
      " Accuracy: 0.9541\n",
      " F1 score: 0.9231\n",
      " F1 full:  [0.97111123 0.87515293]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c66e0b43c954e12897f968d1e1de541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2070\n",
      " Accuracy: 0.9256\n",
      " F1 score: 0.8743\n",
      " F1 full:  [0.95350359 0.79508651]\n",
      "00:05:43.86\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089cb542f16c4652923171377e7bd0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.1053\n",
      " Accuracy: 0.9635\n",
      " F1 score: 0.9383\n",
      " F1 full:  [0.97710188 0.89943264]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571653766ea64d67a3c5dfbd88102d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2435\n",
      " Accuracy: 0.9272\n",
      " F1 score: 0.8729\n",
      " F1 full:  [0.95483453 0.79094959]\n",
      "00:05:44.18\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc6a837333422a989ae72ec1ab0cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training loss: 0.0859\n",
      " Accuracy: 0.9704\n",
      " F1 score: 0.9503\n",
      " F1 full:  [0.9813321  0.91929657]\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa86c3e19fb449ba9263fbdacad5adf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average validation loss: 0.2461\n",
      " Accuracy: 0.9266\n",
      " F1 score: 0.8811\n",
      " F1 full:  [0.95365476 0.80847712]\n",
      "00:05:43.60\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "EPOCHS = 6\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "train_f1 = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_f1 = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    total_train_f1 = 0\n",
    "    f1_full_train = [0,0]\n",
    "    for step, (pair_token, mask, seg, y) in tqdm_notebook(enumerate(train_loader)): \n",
    "        pair_token_ids = pair_token.to(device)\n",
    "        mask_ids = mask.to(device)\n",
    "        seg_ids = seg.to(device)\n",
    "        labels = y.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        prediction = model(pair_token_ids, \n",
    "                                token_type_ids=seg_ids, \n",
    "                                attention_mask=mask_ids\n",
    "                                )\n",
    "        loss = criterion(prediction,labels)\n",
    "        y_pred = torch.log_softmax(prediction, dim=1).argmax(dim=1).cpu().tolist()\n",
    "        y_true = labels.cpu().tolist()\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_full_train = f1_full_train + f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        total_train_acc  += acc\n",
    "        total_train_f1   += f1\n",
    "    \n",
    "    print(\" Average training loss: {0:.4f}\".format(total_train_loss/len(train_loader)))\n",
    "    print(\" Accuracy: {0:.4f}\".format(total_train_acc/len(train_loader)))\n",
    "    print(\" F1 score: {0:.4f}\".format(total_train_f1/len(train_loader)))\n",
    "    print(\" F1 full: \",f1_full_train/len(train_loader))\n",
    "    \n",
    "    train_loss.append(total_train_loss/len(train_loader))\n",
    "    train_acc.append(total_train_acc/len(train_loader))\n",
    "    train_f1.append(total_train_f1/len(train_loader))\n",
    "\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    total_val_f1 = 0\n",
    "    f1_full_val = [0,0]\n",
    "    for step, (pair_token, mask, seg, y) in tqdm_notebook(enumerate(val_loader)): \n",
    "        pair_token_ids = pair_token.to(device)\n",
    "        mask_ids = mask.to(device)\n",
    "        seg_ids = seg.to(device)\n",
    "        labels = y.to(device).long()\n",
    "        with torch.no_grad():\n",
    "            # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "            prediction = model(pair_token_ids, \n",
    "                                token_type_ids=seg_ids, \n",
    "                                attention_mask=mask_ids\n",
    "                                )\n",
    "            \n",
    "            loss = criterion(prediction,labels)\n",
    "            y_pred = torch.log_softmax(prediction, dim=1).argmax(dim=1).cpu().tolist()\n",
    "            y_true = labels.cpu().tolist()\n",
    "\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro')\n",
    "            f1_full_val = f1_full_val + f1_score(y_true, y_pred, average=None)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            total_val_acc  += acc\n",
    "            total_val_f1   += f1\n",
    "\n",
    "    print(\" Average validation loss: {0:.4f}\".format(total_val_loss/len(val_loader)))\n",
    "    print(\" Accuracy: {0:.4f}\".format(total_val_acc/len(val_loader)))\n",
    "    print(\" F1 score: {0:.4f}\".format(total_val_f1/len(val_loader)))\n",
    "    print(\" F1 full: \",f1_full_val/len(val_loader))\n",
    "    val_loss.append(total_val_loss/len(val_loader))\n",
    "    val_acc.append(total_val_acc/len(val_loader))\n",
    "    val_f1.append(total_val_f1/len(val_loader))\n",
    "    \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Model/AspectDetectionModelBaseDataOrigin88.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('QN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6380cd8d6d2424f8fc70282f342af6d0be361f751f7cac84e6bf0d25e635ba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
